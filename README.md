# Construye tu propio Chatbot con RAG
Bienvenido a este breve taller para desplegar tu propio Chatbot usando Retrieval Augmented Generation (RAG) o """Generaci√≥n Aumentada por Recuperaci√≥n""" con Astra DB y con el modelo de conversaci√≥n de OpenAI.

Se aprovecha el uso de [DataStax RAGStack](https://docs.datastax.com/en/ragstack/docs/index.html), que es una colecci√≥n del mejor software open-source para facilitar la implementaci√≥n del patr√≥n RAG en aplicaciones lista para producci√≥n, las cuales usen Astra Vector DB o Apache Cassandra como almacenamiento vectorizado.

![codespace](./assets/chatbot.png)

Que queremos aprender:
- Como aprovechar [DataStax RAGStack](https://docs.datastax.com/en/ragstack/docs/index.html) para usar a nivel productivo los siguientes componenetes:
    - El almacenamiento vectorial de [Astra DB Vector Store](https://db.new) para b√∫squeda de Similaridad Sem√°ntica
    - [LangChain](https://www.langchain.com) para vincular OpenAI y Astra DB
- C√≥mo usar [modelos LLM OpenAI](https://platform.openai.com/docs/models) para chatbots de consulta y respuestas (OpenAI's Large Language Models)
- !Como usar [Streamlit](https://streamlit.io) para desplegar facilmente tu app al internet para que cualquiera pueda usarla! 

## 1Ô∏è‚É£ Prerequisitos
Se asume que ya se tiene acceso a: 
1. [Una cuenta Github](https://github.com)

Adicionalmente se crear√°n accesos de forma gratuita a los siguientes sistemas:
1. [DataStax Astra DB](https://astra.datastax.com) (se puede crear una cuenta a trav√©s de Github)
2. [OpenAI account](https://platform.openai.com/signup) (se puede crear una cuenta a trav√©s de Github)
3. [Streamlit](https://streamlit.io) to deploy your amazing app (se puede crear una cuenta a trav√©s de Github)

Recuerda tener a mano el **Astra DB API Endpoint**, el token **Astra DB ApplicationToken** y la llave **OpenAI API Key**, se usar√°n m√°s adelante donde se indique.


### Registro en Astra DB
Debes crear una basde de datos de vector (vector-capable) de Astra (puedes obtener una gratis en [astra.datastax.com](https://astra.datastax.com))
- Necesitar√°s tener el **API Endpoint** que puedes encontrar en el panel derecho en *Database details*.
- Asegurate de obtener un token **Application Token** para tu base de datos, que tambi√©n puede obtenerse en panel derecho debajo de *Database details*.

![codespace](./assets/astra.png)

### Registo en OpenAI
- Crea una [cuenta OpenAI](https://platform.openai.com/signup) or [entra](https://platform.openai.com/login).
- Navega a la p√°gina [API key page](https://platform.openai.com/account/api-keys) y crea una nueva **Secret Key** (llave secreta), opcionalemente puedes darle un nombre a la llave.

![codespace](./assets/openai-key.png)

### Registro en Streamlit
Sigues los pasos en [here](https://docs.streamlit.io/streamlit-community-cloud/get-started/quickstart).

![codespace](./assets/streamlit.png)

## 2Ô∏è‚É£ Abrir este tutorial en Github Codespaces
Para facilitar las cosas usaremos la fabulosa funcionalidad de Codespace. Github ofrece un experiencia de desarrollo completamente integrada y recursos para comenzar rapidamente: 

1. Abre el repositorio [chatbot-rag-agent](https://github.com/michelderu/build-your-own-rag-agent)
2. Haz click en `Use this template`->`Ceate new repository` como se muestra:

    ![codespace](./assets/create-new-repository.png)

3. Selecciona tu propia cuenta de github y pon un nombre al nuevo repositorio. Puedes colocar idealmente una descripci√≥n. Haz click en `Create repository`

    ![codespace](./assets/repository-name.png)

4. !Genial! ¬°Creaste una copia en tu propia cuenta de Gihub! Ahora a empezar, haz click en `Create codespace on main`:

    ![codespace](./assets/create-codespace.png)

¬°Est√°s listo para codificar! ü•≥  
Codespaces crea tu entorno de desarrollo basado en  `Python 3.11`, automaticamente instalar√° las dependencias de Python a partir del archivo `requirements.txt`. As√≠ que no es necesario hacer nada con `pip install`. Tambi√©n establecera automaticamente el manejo de puertos para poder entrar a apps de Streamlit posteriormente.
Cuando el codespace arraca ejecutara una app de Streamlit, Hello World, que te mostrar√° algunas de las fabulosas capacidades de este framework para UI. Cuando termines de examinarla, simplemente presiona `ctrl-c` en la `terminal` para detenerla.

## 3Ô∏è‚É£ Comenzar a construir una app con Streamlit

Construiremos una aplicaci√≥n con la siguiente arquitectura general:

![steps](./assets/steps.png)

Usaremos Streamlit que un framework sorprendentemente f√°cil de usar para crear el front-end de aplicaciones web.

Lo primero es importar el paquete de streamlit. Llamamos a `st.title` para escribir un t√≠tulo para la p√°gina web, finalmente escribimos un poco de contenido markdown para la p√°gina usando `st.markdown`.

```python
import streamlit as st

# Draw a title and some markdown
st.title("Soporte para la eficiencia personal")
st.markdown("""La Inteligencia Artificial Generativa se considera como el motore de la siguiente revoluci√≥n industrial.  
¬°Los estudios recientes muestran alrededor de **37% de mejora** en la realizaci√≥n del trabajo diario!""")
```

## 4Ô∏è‚É£ Interface del chatbot

El siguiente paso ser√≠a preparar nuestra app para permitir la interacci√≥n como un bot con el usuario. Usamos los siguientes componentes de Streamlit: 

123. `st.chat_input` para permitir que el usuario escriba una pregunta
129. `st.chat_message('human')` para dibujar la entrada del usuario 
133. `st.chat_message('assistant')` para dibujar la respuesta del chatbot

## 5Ô∏è‚É£ Interacci√≥n con el chatbot

 Streamlit ejecuta el c√≥digo cada vez que el usuario interactua con la app, por esa raz√≥n debemos usar caching the datos y recursos dentro de Streamlit, por ejemplo para que una conecci√≥n se establezca una √∫nica vez. 
 
 Necesitamos guardar la interacci√≥n para que en cada redibujado la historia se muestre correctamente.

Para lograrlo hacemos los siguientes pasos:

126. La pregunta se almacena dentro de `st.session_state` en `messages`
146. Almacenamos la respuesta dentro de `st.session_state` en `messages`
119. Cuando la app redibuja, despliega en pantalla toda la historia con un ciclo `for message in st.session_state.messages`

Este manejo funciona porque `session_state` es stateful, o sea guarda su estado, a lo largo la ejecuci√≥n de Streamlit.

Puede notarse que usarmos un diccionario para almacenar tanto al `role` (que puede ser humano o IA) como pregunta o la respuesta ( `question` or `answer`).

Usaremos `@st.cache_data()` y `@st.cache_resource()` para definir el caching. `cache_data` es tipicamente usado para estructuras de datos; `cache_resource` es usado principalmente para los recursos como bases de datos. Adem√°s almacenaremos como recurso la llamada al LLM:

```python
# Cache prompt for future runs
@st.cache_data()
def load_prompt():
    template = """You're a helpful AI assistent tasked to answer the user's questions.
You're friendly and you answer extensively with multiple sentences. You prefer to use bulletpoints to summarize.

CONTEXT:
{context}

QUESTION:
{question}

YOUR ANSWER:"""
    return ChatPromptTemplate.from_messages([("system", template)])
prompt = load_prompt()

# Cache OpenAI Chat Model for future runs
@st.cache_resource()
def load_chat_model():
    return ChatOpenAI(
        temperature=0.3,
        model='gpt-3.5-turbo',
        streaming=True,
        verbose=True
    )
chat_model = load_chat_model()
```

## 6Ô∏è‚É£ Ahora lo m√°s cool! Integraci√≥n con el modelo conversacional de OpenAI

La llamada a la "cadena" o Chain se realiza de la siguiente forma:

```python
# Generate the answer by calling OpenAI's Chat Model
inputs = RunnableMap({
    'question': lambda x: x['question']
})
chain = inputs | prompt | chat_model
response = chain.invoke({'question': question})
answer = response.content
```

Sin embargo, antes de poder continuar, necesitamos proveer la llave de OpenAI (`OPENAI_API_KEY`) en `./streamlit/secrets.toml`. Hay un ejemplo en `secrets.toml.example`:

```toml
# OpenAI secrets
OPENAI_API_KEY = "<YOUR-API-KEY>"
```
## 7Ô∏è‚É£ Conectar con the Astra DB Vector Store para agregar contexto

Hasta este punto lo que a√∫n hace falta revisar en la integraci√≥n con la base de datos vectoria, Astra DB Vector store, para que podamos tener respuestas contextualizadas. Al integrar nuestra app con Astra DB Vector Store podemos proveer contexto en tiempo real para el modelo conversacional del LLM. Los pasos para implementar RAG (Retrieval Augmented Generation) son: 
1. El usuario hace una pregunta
2. Una b√∫squeda por similaridad sem√°ntica se ejecuta en Astra DB Vector Store
3. El contexto recuperado se provee al Promt para el modelo conversacional
4. El modelo conversacional regresa una respuesta, que toma en cuenta el contexto recuperado

Para lograr estos pasos, debemos primero establecer una conecci√≥n a nuestra Astra DB Vector Store:

```python
# Cache the Astra DB Vector Store for future runs
@st.cache_resource(show_spinner='Connecting to Astra')
def load_retriever():
    # Connect to the Vector Store
    vector_store = AstraDB(
        embedding=OpenAIEmbeddings(),
        collection_name="my_store",
        api_endpoint=st.secrets['ASTRA_API_ENDPOINT'],
        token=st.secrets['ASTRA_TOKEN']
    )

    # Get the retriever for the Chat Model
    retriever = vector_store.as_retriever(
        search_kwargs={"k": 5}
    )
    return retriever
retriever = load_retriever()
```

Lo √∫nico que faltar√≠a es que la cadena o Chain para incluir la llamada al almacenamiento vectoriol:

```python
# Generate the answer by calling OpenAI's Chat Model
137. inputs = RunnableMap({
138.    'context': lambda x: retriever.get_relevant_documents(x['question']),
139.    'question': lambda x: x['question']
140. })
```

Para terminar, debemos tener el endpoint de nuestra base de datos y su token de seguridad, `ASTRA_API_ENDPOINT` y `ASTRA_TOKEN`, los cuales colocaremos en `./streamlit/secrets.toml`.

```toml
# Astra DB secrets
ASTRA_API_ENDPOINT = "<YOUR-API-ENDPOINT>"
ASTRA_TOKEN = "<YOUR-TOKEN>"
```

## 8Ô∏è‚É£ Hacer que la aplicaci√≥n tenga streaming

Ser√≠a muy interesante que la respuesta vaya apareciendo en la pantalla conforme est√° siendo generada. Bueno eso es sencillo! 

Es necesario crear un Call Back de Streaming que se llame en la generaci√≥n de cada token:  

```python
# Streaming call back handler for responses
15. class StreamHandler(BaseCallbackHandler):
16.     def __init__(self, container, initial_text=""):
17.         self.container = container
18.         self.text = initial_text
19. 
20.     def on_llm_new_token(self, token: str, **kwargs):
21.         self.text += token
22.         self.container.markdown(self.text + "‚ñå")
```

Y solicitamos al modelo conversacional que haga uso del StreamHandler:

```python
142. response = chain.invoke({'question': question}, config={'callbacks': [StreamHandler(response_placeholder)]})
```

El `response_placeholer` en el c√≥digo de arriba define el lugar donde los tokens se escribir√°n. Ese espacio est√° creado llamando `st.empty()` como sigue:

```python
# UI placeholder to start filling with agent response
133. with st.chat_message('assistant'):
134.     response_placeholder = st.empty()
```

Con esto podemos ver la respuesta siendo escrita en tiempo real en la ventana del navegador.

## 9Ô∏è‚É£ Ahora agregamos el contexto adicional

La meta final es que podamos agregar el contexto de nuestra compa√±√≠a al agente de chat. Para lograr esto, agregamos un control para subir archivos de tipo PDF, los cuales ser√°n usados para regresar una respuesta significativa y con contexto!

El formado para subir el archivo es f√°cil de crear en Streamlit:

```python
110. # Include the upload form for new data to be Vectorized
111. with st.sidebar:
112.     with st.form('upload'):
113.         uploaded_file = st.file_uploader('Upload a document for additional context', type=['pdf'])
114.         submitted = st.form_submit_button('Save to Astra DB')
115.         if submitted:
116.             vectorize_text(uploaded_file)
```

Usamos una funci√≥n para cargar el PDF y hacer la ingesta en Astra DB a la misma vez que se vectoriza su contenido.

```python
# Function for Vectorizing uploaded data into Astra DB
def vectorize_text(uploaded_file, vector_store):
    if uploaded_file is not None:
        
        # Write to temporary file
        temp_dir = tempfile.TemporaryDirectory()
        file = uploaded_file
        temp_filepath = os.path.join(temp_dir.name, file.name)
        with open(temp_filepath, 'wb') as f:
            f.write(file.getvalue())

        # Load the PDF
        docs = []
        loader = PyPDFLoader(temp_filepath)
        docs.extend(loader.load())

        # Create the text splitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size = 1500,
            chunk_overlap  = 100
        )

        # Vectorize the PDF and load it into the Astra DB Vector Store
        pages = text_splitter.split_documents(docs)
        vector_store.add_documents(pages)  
        st.info(f"{len(pages)} pages loaded.")
```

## 1Ô∏è‚É£0Ô∏è‚É£ Probemos nuestro agente con RAG!

Para ejecutar nuestra aplicaci√≥n, debemos introducir la siguiente instrucci√≥n en la consola:
```bash
streamlit run app_chat.py
```

Si hay todav√≠a est√° ejecutandose alguna aplicaci√≥n, s√≥lo hay que detenerla con `ctrl-c` antes de ejecutar nuestra nueva app.

En la ventana del navegador embedido veremos la UI de nuestro chatbot. Ahora debemos cargar un documento PDF, o varios, cuantos m√°s mejor; los cuales sean relevantes, para comenzar a hacer preguntas acerca del tema. ¬°Ver√°s que las respuestas ser√°n m√°s relevantes, asi como con mayor sentido y contexto!

![end-result](./assets/end-result.png)

## 1Ô∏è‚É£1Ô∏è‚É£ Finalmente hay que desplegar nuestra app a la nube de Streamlit!

El √∫ltimo paso es desplegar nuestra fant√°stica aplicac√≥n al Internet para que otros puedan ver tu trabajo y su funcionalidad. 

### Crea tu cuenta de Streamlit 
Si no lo has hecho anteriormente, crea y configura una cuenta en Streamlit:

1. Ve a [Streamlit.io](https://streamlit.io) y haz click en `Sign up`. Luego selecciona `Continue with Github`:

    ![Streamlit](./assets/streamlit-0.png)

2. Accesa usando tus credenciales de GitHub:

    ![Streamlit](./assets/streamlit-1.png)

3. Dale autorizati√≥n a Streamlit:

    ![Streamlit](./assets/streamlit-2.png)

4. Y configura tu cuenta:

    ![Streamlit](./assets/streamlit-3.png)

### Despliega tu app

En la pantalla principal, una vez adentro, haz click en `New app`.

1. Cuando se trate de tu primer despliegue, deber√°s proveer permisos adicionales:

    ![Streamlit](./assets/streamlit-4.png)

2. Ahora define la configuraci√≥n de tu aplicaci√≥n. Usar el nombre de tu repositorio, y la ruta de tu archivo principal como `app_chat.py`. Selecciona una URL de app interesante, pues la aplicaci√≥n ser√° desplegada en esa ruta: 

    ![Streamlit](./assets/streamlit-5.png)

3. Haz click en Advanced, selecciona Python 3.11 y, copia y pega el contenido de tu archivo `secrets.toml`.

Haz click en Deploy! Espera unos instantes y tu app estar√° en l√≠nea para todos! 

‚õîÔ∏è Toma en cuenta y se pruedente con el hecho de es una app p√∫blica que ocupa tu cuenta de OpenAI, lo que generar√° costos. Probablemente querr√°s escudar tu app haciendo click en `Settings->Sharing` en la pantalla principal y definir algunas direcciones de correo electr√≥nicoo a las que se les permita el acceso. Para habilitar esta opci√≥n, debes asociar tu cuenta de Google. 


